{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 04 - LDA Topic Modeling\n",
                "\n",
                "This notebook trains the LDA (Latent Dirichlet Allocation) topic model.\n",
                "\n",
                "## Steps\n",
                "- Load processed corpus\n",
                "- Create dictionary and corpus\n",
                "- Find optimal number of topics (coherence-based)\n",
                "- Train final model\n",
                "- Evaluate and save model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import sys\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd().parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src.config import get_settings, ensure_directories\n",
                "from src.lda_model import LDATopicModel, find_optimal_topics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load settings\n",
                "settings = get_settings()\n",
                "ensure_directories(settings)\n",
                "\n",
                "# Load processed corpus\n",
                "corpus_path = settings.processed_data_dir / settings.processed_corpus_file\n",
                "print(f\"Loading processed corpus from: {corpus_path}\")\n",
                "\n",
                "with open(corpus_path, 'rb') as f:\n",
                "    corpus_data = pickle.load(f)\n",
                "\n",
                "processed_docs = corpus_data['documents']\n",
                "df = corpus_data['dataframe']\n",
                "\n",
                "print(f\"Loaded {len(processed_docs):,} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Explore Corpus Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Corpus statistics\n",
                "all_tokens = [t for doc in processed_docs for t in doc]\n",
                "unique_tokens = set(all_tokens)\n",
                "\n",
                "print(\"Corpus Statistics:\")\n",
                "print(\"-\" * 40)\n",
                "print(f\"Documents: {len(processed_docs):,}\")\n",
                "print(f\"Total tokens: {len(all_tokens):,}\")\n",
                "print(f\"Unique tokens: {len(unique_tokens):,}\")\n",
                "print(f\"Avg tokens/doc: {len(all_tokens)/len(processed_docs):.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Find Optimal Number of Topics\n",
                "\n",
                "We'll train models with different numbers of topics and evaluate using coherence score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration for topic search\n",
                "TOPIC_RANGE = range(5, 21, 2)  # 5, 7, 9, 11, 13, 15, 17, 19\n",
                "\n",
                "# Set to True to run the search (can take a long time)\n",
                "RUN_TOPIC_SEARCH = True\n",
                "\n",
                "# If you already know the optimal number of topics, set it here\n",
                "FIXED_NUM_TOPICS = None  # e.g., 10\n",
                "\n",
                "print(f\"Topic range to test: {list(TOPIC_RANGE)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if RUN_TOPIC_SEARCH and FIXED_NUM_TOPICS is None:\n",
                "    print(\"Finding optimal number of topics...\")\n",
                "    print(\"This may take a while...\")\n",
                "    print(\"-\" * 50)\n",
                "    \n",
                "    optimal_topics, search_results = find_optimal_topics(\n",
                "        processed_docs,\n",
                "        topic_range=TOPIC_RANGE,\n",
                "        settings=settings,\n",
                "        show_progress=True,\n",
                "    )\n",
                "    \n",
                "    print(f\"\\n‚úÖ Optimal number of topics: {optimal_topics}\")\n",
                "else:\n",
                "    optimal_topics = FIXED_NUM_TOPICS or settings.lda_num_topics\n",
                "    search_results = None\n",
                "    print(f\"Using fixed number of topics: {optimal_topics}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot coherence scores\n",
                "if search_results:\n",
                "    fig, ax = plt.subplots(figsize=(10, 6))\n",
                "    \n",
                "    topics = [r['num_topics'] for r in search_results]\n",
                "    coherences = [r['coherence'] for r in search_results]\n",
                "    \n",
                "    ax.plot(topics, coherences, 'o-', markersize=8, linewidth=2)\n",
                "    ax.axvline(optimal_topics, color='red', linestyle='--', \n",
                "               label=f'Optimal: {optimal_topics}')\n",
                "    \n",
                "    ax.set_xlabel('Number of Topics')\n",
                "    ax.set_ylabel('Coherence Score (c_v)')\n",
                "    ax.set_title('Topic Count vs Coherence Score', fontweight='bold')\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Save results\n",
                "    results_df = pd.DataFrame(search_results)\n",
                "    results_path = settings.outputs_dir / 'topic_coherence_results.csv'\n",
                "    results_df.to_csv(results_path, index=False)\n",
                "    print(f\"Saved search results to: {results_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train Final Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model configuration\n",
                "NUM_TOPICS = optimal_topics  # Use optimal from search\n",
                "PASSES = 15\n",
                "ITERATIONS = 400\n",
                "WORKERS = 4  # Number of CPU cores to use\n",
                "\n",
                "print(f\"Training final model with configuration:\")\n",
                "print(f\"  Topics: {NUM_TOPICS}\")\n",
                "print(f\"  Passes: {PASSES}\")\n",
                "print(f\"  Iterations: {ITERATIONS}\")\n",
                "print(f\"  Workers: {WORKERS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train model\n",
                "print(\"\\nTraining LDA model...\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "model = LDATopicModel(settings)\n",
                "model.train(\n",
                "    processed_docs,\n",
                "    num_topics=NUM_TOPICS,\n",
                "    passes=PASSES,\n",
                "    iterations=ITERATIONS,\n",
                "    workers=WORKERS,\n",
                "    show_progress=True,\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display model metadata\n",
                "if model.metadata:\n",
                "    meta = model.metadata\n",
                "    print(\"\\nModel Metadata:\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"Number of topics:    {meta.num_topics}\")\n",
                "    print(f\"Number of documents: {meta.num_documents:,}\")\n",
                "    print(f\"Vocabulary size:     {meta.vocabulary_size:,}\")\n",
                "    print(f\"Coherence score:     {meta.coherence_score:.4f}\")\n",
                "    print(f\"Training time:       {meta.training_time_seconds:.1f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Explore Topics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display all topics\n",
                "topics = model.get_topics(num_words=15)\n",
                "\n",
                "print(f\"\\n{len(topics)} Topics Discovered:\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "for topic in topics:\n",
                "    words = ', '.join(topic.top_words[:10])\n",
                "    print(f\"\\nTopic {topic.topic_id}:\")\n",
                "    print(f\"  {words}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize topic words\n",
                "fig, axes = plt.subplots(2, min(5, NUM_TOPICS//2 + 1), figsize=(20, 8))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, topic in enumerate(topics[:10]):\n",
                "    if i >= len(axes):\n",
                "        break\n",
                "    \n",
                "    words = [w for w, _ in topic.words[:10]]\n",
                "    weights = [w for _, w in topic.words[:10]]\n",
                "    \n",
                "    ax = axes[i]\n",
                "    ax.barh(range(len(words)), weights, color=plt.cm.tab10(i))\n",
                "    ax.set_yticks(range(len(words)))\n",
                "    ax.set_yticklabels(words)\n",
                "    ax.invert_yaxis()\n",
                "    ax.set_title(f'Topic {topic.topic_id}', fontweight='bold')\n",
                "\n",
                "# Hide unused subplots\n",
                "for i in range(len(topics), len(axes)):\n",
                "    axes[i].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Document-Topic Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get topic distribution for all documents\n",
                "print(\"Computing document-topic distributions...\")\n",
                "topic_matrix = model.get_document_topic_matrix(processed_docs, show_progress=True)\n",
                "\n",
                "print(f\"\\nMatrix shape: {topic_matrix.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Topic prevalence\n",
                "topic_prevalence = topic_matrix.mean(axis=0)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "bars = ax.bar(range(NUM_TOPICS), topic_prevalence, color=plt.cm.tab10.colors[:NUM_TOPICS])\n",
                "ax.set_xlabel('Topic')\n",
                "ax.set_ylabel('Average Probability')\n",
                "ax.set_title('Topic Prevalence Across Documents', fontweight='bold')\n",
                "ax.set_xticks(range(NUM_TOPICS))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample document topic assignments\n",
                "print(\"\\nSample Document-Topic Assignments:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for i in range(min(5, len(df))):\n",
                "    dominant_topic = np.argmax(topic_matrix[i])\n",
                "    prob = topic_matrix[i, dominant_topic]\n",
                "    title = df.iloc[i]['title'][:60]\n",
                "    print(f\"Doc {i}: Topic {dominant_topic} ({prob:.2f}) - {title}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model and artifacts\n",
                "model_dir = model.save()\n",
                "\n",
                "print(f\"\\n‚úÖ Model saved to: {model_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Also save topic-document matrix for dashboard\n",
                "matrix_path = settings.processed_data_dir / settings.topic_document_matrix_file\n",
                "\n",
                "# Create DataFrame with topic columns\n",
                "df_topics = df.copy()\n",
                "for i in range(NUM_TOPICS):\n",
                "    df_topics[f'topic_{i}'] = topic_matrix[:, i]\n",
                "\n",
                "df_topics['dominant_topic'] = np.argmax(topic_matrix, axis=1)\n",
                "df_topics['dominant_prob'] = np.max(topic_matrix, axis=1)\n",
                "\n",
                "# Remove tokens column (too large for CSV)\n",
                "if 'tokens' in df_topics.columns:\n",
                "    df_topics = df_topics.drop(columns=['tokens'])\n",
                "\n",
                "df_topics.to_csv(matrix_path, index=False)\n",
                "print(f\"‚úÖ Topic-document matrix saved to: {matrix_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"LDA MODELING COMPLETE\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nüìä Topics: {NUM_TOPICS}\")\n",
                "print(f\"üìà Coherence: {model.metadata.coherence_score:.4f}\")\n",
                "print(f\"üìö Documents: {len(processed_docs):,}\")\n",
                "print(f\"üìù Vocabulary: {model.metadata.vocabulary_size:,}\")\n",
                "\n",
                "print(f\"\\nüìÅ Model artifacts saved to: {model_dir}\")\n",
                "\n",
                "print(f\"\\nüëâ Next: Run 05_analysis_visualization.ipynb for detailed analysis\")\n",
                "print(f\"   Then: Launch the Streamlit dashboard with 'streamlit run dashboard/app.py'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}