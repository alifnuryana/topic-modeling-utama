{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Cleaning\n",
    "\n",
    "This notebook cleans the raw metadata based on EDA findings.\n",
    "\n",
    "## Cleaning Steps\n",
    "- Handle missing values\n",
    "- Remove duplicates\n",
    "- Standardize date formats\n",
    "- Validate and normalize text fields\n",
    "- Filter out unusable records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.552692700Z",
     "start_time": "2025-12-13T08:30:03.835130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.config import get_settings, ensure_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.845102900Z",
     "start_time": "2025-12-13T08:30:04.554700300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data from: c:\\Users\\alifn\\Code\\topic-modeling-utama\\data\\raw\\raw_metadata.csv\n",
      "Loaded 12,647 records\n",
      "Columns: ['identifier', 'title', 'abstract', 'authors', 'date', 'subjects', 'publisher', 'types', 'language', 'source']\n"
     ]
    }
   ],
   "source": [
    "# Load settings and data\n",
    "settings = get_settings()\n",
    "ensure_directories(settings)\n",
    "\n",
    "data_path = settings.raw_data_dir / settings.raw_metadata_file\n",
    "print(f\"Loading raw data from: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.863245500Z",
     "start_time": "2025-12-13T08:30:04.848116200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial record count: 12,647\n"
     ]
    }
   ],
   "source": [
    "# Initial state\n",
    "initial_count = len(df)\n",
    "print(f\"\\nInitial record count: {initial_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.901043400Z",
     "start_time": "2025-12-13T08:30:04.865249500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 exact duplicates\n",
      "Removed 0 duplicate identifiers\n"
     ]
    }
   ],
   "source": [
    "# Remove exact duplicates\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after:,} exact duplicates\")\n",
    "\n",
    "# Remove duplicate identifiers (keep first)\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=['identifier'], keep='first')\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after:,} duplicate identifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.924851700Z",
     "start_time": "2025-12-13T08:30:04.902737400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 125 duplicate titles\n",
      "Removed 434 duplicate abstracts\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate titles (more aggressive - optional)\n",
    "# Uncomment if you want to remove records with duplicate titles\n",
    "\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=['title'], keep='first')\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after:,} duplicate titles\")\n",
    "\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=['abstract'], keep='first')\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after:,} duplicate abstracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.951727200Z",
     "start_time": "2025-12-13T08:30:04.926857500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current missing values:\n",
      "  abstract: 1 (0.0%)\n",
      "  authors: 7 (0.1%)\n",
      "  subjects: 1,213 (10.0%)\n",
      "  publisher: 89 (0.7%)\n",
      "  types: 147 (1.2%)\n",
      "  language: 2,335 (19.3%)\n",
      "  source: 12,088 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check current missing values\n",
    "print(\"Current missing values:\")\n",
    "for col in df.columns:\n",
    "    missing = df[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"  {col}: {missing:,} ({missing/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.973272600Z",
     "start_time": "2025-12-13T08:30:04.952725300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 records without title\n"
     ]
    }
   ],
   "source": [
    "# Remove records without title (essential field)\n",
    "before = len(df)\n",
    "df = df[df['title'].notna() & (df['title'].str.strip() != '')]\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after:,} records without title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:04.997271800Z",
     "start_time": "2025-12-13T08:30:04.974271100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1 records without abstract\n"
     ]
    }
   ],
   "source": [
    "# Remove records without abstract (needed for topic modeling)\n",
    "before = len(df)\n",
    "df = df[df['abstract'].notna() & (df['abstract'].str.strip() != '')]\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after:,} records without abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:05.023261700Z",
     "start_time": "2025-12-13T08:30:04.999272200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values for non-essential fields\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values for non-essential fields\n",
    "df['authors'] = df['authors'].fillna('Tidak Diketahui')\n",
    "df['subjects'] = df['subjects'].fillna('Tidak Diketahui')\n",
    "df['publisher'] = df['publisher'].fillna('Tidak Diketahui')\n",
    "df['types'] = df['types'].fillna('Tidak Diketahui')\n",
    "df['language'] = df['language'].fillna('Tidak Diketahui')\n",
    "df['source'] = df['source'].fillna('Tidak Diketahui')\n",
    "\n",
    "print(\"Filled missing values for non-essential fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standardize Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:08.654608700Z",
     "start_time": "2025-12-13T08:30:05.024259500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed dates: 12,087 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Parse and standardize dates\n",
    "def parse_date(date_str):\n",
    "    \"\"\"Parse various date formats and extract year.\"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return None, None\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Try to parse as datetime\n",
    "    try:\n",
    "        parsed = pd.to_datetime(date_str)\n",
    "        return parsed, parsed.year\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Extract year using regex\n",
    "    year_match = re.search(r'(19|20)\\d{2}', date_str)\n",
    "    if year_match:\n",
    "        year = int(year_match.group())\n",
    "        return pd.Timestamp(year=year, month=1, day=1), year\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Apply parsing\n",
    "parsed_dates = df['date'].apply(parse_date)\n",
    "df['parsed_date'] = [d[0] for d in parsed_dates]\n",
    "df['year'] = [d[1] for d in parsed_dates]\n",
    "\n",
    "# Statistics\n",
    "valid_dates = df['year'].notna().sum()\n",
    "print(f\"Successfully parsed dates: {valid_dates:,} ({valid_dates/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:08.735270Z",
     "start_time": "2025-12-13T08:30:08.714928900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year distribution:\n",
      "year\n",
      "2016    1041\n",
      "2017     932\n",
      "2018     845\n",
      "2019     729\n",
      "2020     795\n",
      "2021    1515\n",
      "2022     880\n",
      "2023     327\n",
      "2024     564\n",
      "2025     385\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Year distribution\n",
    "print(\"\\nYear distribution:\")\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "print(year_counts.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:09.513846200Z",
     "start_time": "2025-12-13T08:30:08.735270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied text normalization to title, abstract, authors, subjects\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text field.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove control characters\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply normalization\n",
    "df['title'] = df['title'].apply(normalize_text)\n",
    "df['abstract'] = df['abstract'].apply(normalize_text)\n",
    "df['authors'] = df['authors'].apply(normalize_text)\n",
    "df['subjects'] = df['subjects'].apply(normalize_text)\n",
    "\n",
    "print(\"Applied text normalization to title, abstract, authors, subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter Records by Abstract Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:09.722019200Z",
     "start_time": "2025-12-13T08:30:09.526361200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract word count statistics:\n",
      "count    12087.000000\n",
      "mean       215.682386\n",
      "std         77.607181\n",
      "min          1.000000\n",
      "25%        160.000000\n",
      "50%        201.000000\n",
      "75%        261.000000\n",
      "max       1700.000000\n",
      "Name: abstract_word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate word counts\n",
    "df['abstract_word_count'] = df['abstract'].str.split().str.len()\n",
    "\n",
    "print(\"Abstract word count statistics:\")\n",
    "print(df['abstract_word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:09.745081300Z",
     "start_time": "2025-12-13T08:30:09.723020300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 10 records with abstract < 20 words\n"
     ]
    }
   ],
   "source": [
    "# Remove very short abstracts\n",
    "MIN_ABSTRACT_WORDS = 20\n",
    "\n",
    "before = len(df)\n",
    "df = df[df['abstract_word_count'] >= MIN_ABSTRACT_WORDS]\n",
    "after = len(df)\n",
    "\n",
    "print(f\"Removed {before - after:,} records with abstract < {MIN_ABSTRACT_WORDS} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:09.774813900Z",
     "start_time": "2025-12-13T08:30:09.747086600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns: ['identifier', 'title', 'abstract', 'authors', 'date', 'year', 'subjects', 'publisher', 'types', 'language']\n"
     ]
    }
   ],
   "source": [
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Select final columns\n",
    "final_columns = [\n",
    "    'identifier',\n",
    "    'title',\n",
    "    'abstract',\n",
    "    'authors',\n",
    "    'date',\n",
    "    'year',\n",
    "    'subjects',\n",
    "    'publisher',\n",
    "    'types',\n",
    "    'language',\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "final_columns = [c for c in final_columns if c in df.columns]\n",
    "df_clean = df[final_columns].copy()\n",
    "\n",
    "print(f\"Final columns: {list(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:09.796136500Z",
     "start_time": "2025-12-13T08:30:09.776298600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLEANING SUMMARY\n",
      "============================================================\n",
      "Initial records:  12,647\n",
      "Final records:    12,077\n",
      "Removed:          570 (4.5%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Initial records:  {initial_count:,}\")\n",
    "print(f\"Final records:    {len(df_clean):,}\")\n",
    "print(f\"Removed:          {initial_count - len(df_clean):,} ({(initial_count - len(df_clean))/initial_count*100:.1f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:10.184071100Z",
     "start_time": "2025-12-13T08:30:09.806639400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved cleaned data to: c:\\Users\\alifn\\Code\\topic-modeling-utama\\data\\processed\\clean_metadata.csv\n",
      "ðŸ“Š Total records: 12,077\n"
     ]
    }
   ],
   "source": [
    "# Save to processed directory\n",
    "output_path = settings.processed_data_dir / settings.clean_metadata_file\n",
    "\n",
    "df_clean.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… Saved cleaned data to: {output_path}\")\n",
    "print(f\"ðŸ“Š Total records: {len(df_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:10.210607800Z",
     "start_time": "2025-12-13T08:30:10.184071100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>subjects</th>\n",
       "      <th>publisher</th>\n",
       "      <th>types</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oai:repository.widyatama.ac.id:123456789/14397</td>\n",
       "      <td>PENGARUH BUDAYA KESELAMATAN DAN KESEHATAN KERJ...</td>\n",
       "      <td>Tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "      <td>Falyana, Diki Hendra</td>\n",
       "      <td>2022-01-05T05:10:37Z</td>\n",
       "      <td>2022</td>\n",
       "      <td>budaya keselamatan dan kesehatan (K3); prosedu...</td>\n",
       "      <td>Program Studi Manajemen S1 Universitas Widyatama</td>\n",
       "      <td>Thesis</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oai:repository.widyatama.ac.id:123456789/859</td>\n",
       "      <td>Pengaruh Kompensasi terhadap Motivasi Kerja Ka...</td>\n",
       "      <td>Skripsi ini disusun oleh Andri Tanjung, NRP 02...</td>\n",
       "      <td>Tanjung, Andri</td>\n",
       "      <td>2009-03-11T02:35:44Z</td>\n",
       "      <td>2009</td>\n",
       "      <td>Pengaruh Kompensasi terhadap Motivasi Kerja Ka...</td>\n",
       "      <td>Universitas Widyatama</td>\n",
       "      <td>Thesis</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oai:repository.widyatama.ac.id:123456789/5337</td>\n",
       "      <td>PERANAN SISTEM INFORMASI AKUNTANSI DALAM MENUN...</td>\n",
       "      <td>Setiap organisasi didirikan untuk mencapai tuj...</td>\n",
       "      <td>Setiawan, David</td>\n",
       "      <td>2015-06-17T06:18:20Z</td>\n",
       "      <td>2015</td>\n",
       "      <td>Sistem Informasi Akuntansi; Pengendalian Inter...</td>\n",
       "      <td>Universitas Widyatama</td>\n",
       "      <td>Thesis</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oai:repository.widyatama.ac.id:123456789/107890</td>\n",
       "      <td>PENGARUH USIA DAN MASA KERJA TERHADAP PRODUKTI...</td>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui peng...</td>\n",
       "      <td>Fauzia, Galih Eza</td>\n",
       "      <td>2024-04-25T03:35:03Z</td>\n",
       "      <td>2024</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>Thesis</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oai:repository.widyatama.ac.id:123456789/8665</td>\n",
       "      <td>PENGARUH SISTEM PENGENDALIAN INTERNAL PEMERINT...</td>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui peng...</td>\n",
       "      <td>Aruan, Hicca Maria Gandi Putri</td>\n",
       "      <td>2017-10-18T23:53:16Z</td>\n",
       "      <td>2017</td>\n",
       "      <td>Sistem Pengendalian Internal Pemerintah; Kuali...</td>\n",
       "      <td>Universitas Widyatama</td>\n",
       "      <td>Thesis</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        identifier  \\\n",
       "0   oai:repository.widyatama.ac.id:123456789/14397   \n",
       "1     oai:repository.widyatama.ac.id:123456789/859   \n",
       "2    oai:repository.widyatama.ac.id:123456789/5337   \n",
       "3  oai:repository.widyatama.ac.id:123456789/107890   \n",
       "4    oai:repository.widyatama.ac.id:123456789/8665   \n",
       "\n",
       "                                               title  \\\n",
       "0  PENGARUH BUDAYA KESELAMATAN DAN KESEHATAN KERJ...   \n",
       "1  Pengaruh Kompensasi terhadap Motivasi Kerja Ka...   \n",
       "2  PERANAN SISTEM INFORMASI AKUNTANSI DALAM MENUN...   \n",
       "3  PENGARUH USIA DAN MASA KERJA TERHADAP PRODUKTI...   \n",
       "4  PENGARUH SISTEM PENGENDALIAN INTERNAL PEMERINT...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Tujuan penelitian ini adalah untuk mengetahui ...   \n",
       "1  Skripsi ini disusun oleh Andri Tanjung, NRP 02...   \n",
       "2  Setiap organisasi didirikan untuk mencapai tuj...   \n",
       "3  Penelitian ini bertujuan untuk mengetahui peng...   \n",
       "4  Penelitian ini bertujuan untuk mengetahui peng...   \n",
       "\n",
       "                          authors                  date  year  \\\n",
       "0            Falyana, Diki Hendra  2022-01-05T05:10:37Z  2022   \n",
       "1                  Tanjung, Andri  2009-03-11T02:35:44Z  2009   \n",
       "2                 Setiawan, David  2015-06-17T06:18:20Z  2015   \n",
       "3               Fauzia, Galih Eza  2024-04-25T03:35:03Z  2024   \n",
       "4  Aruan, Hicca Maria Gandi Putri  2017-10-18T23:53:16Z  2017   \n",
       "\n",
       "                                            subjects  \\\n",
       "0  budaya keselamatan dan kesehatan (K3); prosedu...   \n",
       "1  Pengaruh Kompensasi terhadap Motivasi Kerja Ka...   \n",
       "2  Sistem Informasi Akuntansi; Pengendalian Inter...   \n",
       "3                                    Tidak Diketahui   \n",
       "4  Sistem Pengendalian Internal Pemerintah; Kuali...   \n",
       "\n",
       "                                          publisher   types language  \n",
       "0  Program Studi Manajemen S1 Universitas Widyatama  Thesis    other  \n",
       "1                             Universitas Widyatama  Thesis    other  \n",
       "2                             Universitas Widyatama  Thesis    other  \n",
       "3                                   Tidak Diketahui  Thesis    other  \n",
       "4                             Universitas Widyatama  Thesis    other  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview cleaned data\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:10.444426Z",
     "start_time": "2025-12-13T08:30:10.242058800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data statistics:\n",
      "----------------------------------------\n",
      "Records: 12,077\n",
      "Date range: 2007 - 2025\n",
      "Avg abstract words: 216\n"
     ]
    }
   ],
   "source": [
    "# Quick stats\n",
    "print(\"\\nCleaned data statistics:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Records: {len(df_clean):,}\")\n",
    "print(f\"Date range: {df_clean['year'].min():.0f} - {df_clean['year'].max():.0f}\")\n",
    "print(f\"Avg abstract words: {df_clean['abstract'].str.split().str.len().mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T08:30:10.569273Z",
     "start_time": "2025-12-13T08:30:10.537867400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘‰ Next: Run 02b_eda_clean_data.ipynb to analyze the cleaned data\n",
      "   Then: Run 03_preprocessing.ipynb for text preprocessing\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ‘‰ Next: Run 02b_eda_clean_data.ipynb to analyze the cleaned data\")\n",
    "print(f\"   Then: Run 03_preprocessing.ipynb for text preprocessing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
