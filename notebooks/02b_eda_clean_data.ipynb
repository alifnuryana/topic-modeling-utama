{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02b - Exploratory Data Analysis: Cleaned Data\n",
                "\n",
                "This notebook performs EDA on the cleaned data to:\n",
                "- Verify cleaning effectiveness\n",
                "- Analyze final dataset statistics\n",
                "- Inform preprocessing decisions (stopwords, min/max length)\n",
                "- Understand temporal patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import sys\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "import re\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd().parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src.config import get_settings\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load cleaned data\n",
                "settings = get_settings()\n",
                "data_path = settings.processed_data_dir / settings.clean_metadata_file\n",
                "\n",
                "print(f\"Loading cleaned data from: {data_path}\")\n",
                "df = pd.read_csv(data_path)\n",
                "print(f\"Loaded {len(df):,} records\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Dataset Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic info\n",
                "print(\"Dataset Shape:\", df.shape)\n",
                "print(\"\\nColumn info:\")\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify no critical missing values\n",
                "print(\"Missing values check:\")\n",
                "print(\"-\" * 40)\n",
                "for col in ['title', 'abstract']:\n",
                "    missing = df[col].isna().sum()\n",
                "    print(f\"{col}: {missing} missing\")\n",
                "\n",
                "if df['title'].isna().sum() == 0 and df['abstract'].isna().sum() == 0:\n",
                "    print(\"\\nâœ… No missing values in critical fields!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample records\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Length Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate text statistics\n",
                "df['title_words'] = df['title'].str.split().str.len()\n",
                "df['abstract_words'] = df['abstract'].str.split().str.len()\n",
                "df['abstract_chars'] = df['abstract'].str.len()\n",
                "\n",
                "print(\"Title word count statistics:\")\n",
                "print(df['title_words'].describe())\n",
                "\n",
                "print(\"\\nAbstract word count statistics:\")\n",
                "print(df['abstract_words'].describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize distributions\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Title words\n",
                "axes[0].hist(df['title_words'], bins=30, edgecolor='white', alpha=0.7, color='steelblue')\n",
                "axes[0].axvline(df['title_words'].median(), color='red', linestyle='--', \n",
                "                label=f'Median: {df[\"title_words\"].median():.0f}')\n",
                "axes[0].set_xlabel('Word Count')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].set_title('Title Word Count Distribution', fontweight='bold')\n",
                "axes[0].legend()\n",
                "\n",
                "# Abstract words\n",
                "axes[1].hist(df['abstract_words'], bins=50, edgecolor='white', alpha=0.7, color='coral')\n",
                "axes[1].axvline(df['abstract_words'].median(), color='red', linestyle='--',\n",
                "                label=f'Median: {df[\"abstract_words\"].median():.0f}')\n",
                "axes[1].set_xlabel('Word Count')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "axes[1].set_title('Abstract Word Count Distribution', fontweight='bold')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Temporal Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Year distribution\n",
                "if 'year' in df.columns:\n",
                "    year_counts = df['year'].value_counts().sort_index()\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(14, 6))\n",
                "    \n",
                "    bars = ax.bar(year_counts.index, year_counts.values, edgecolor='white', color='teal')\n",
                "    ax.set_xlabel('Year')\n",
                "    ax.set_ylabel('Number of Publications')\n",
                "    ax.set_title('Publications by Year', fontweight='bold')\n",
                "    \n",
                "    # Highlight recent years\n",
                "    for bar in bars[-5:]:\n",
                "        bar.set_color('coral')\n",
                "    \n",
                "    plt.xticks(rotation=45)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nYear range: {df['year'].min():.0f} - {df['year'].max():.0f}\")\n",
                "    print(f\"Most productive year: {year_counts.idxmax():.0f} ({year_counts.max()} publications)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Word Frequency Analysis (Pre-preprocessing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple tokenization for frequency analysis\n",
                "def simple_tokenize(text):\n",
                "    \"\"\"Simple tokenization for frequency analysis.\"\"\"\n",
                "    if pd.isna(text):\n",
                "        return []\n",
                "    text = str(text).lower()\n",
                "    # Remove punctuation and split\n",
                "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text)\n",
                "    return words\n",
                "\n",
                "# Collect all words from abstracts\n",
                "all_words = []\n",
                "for abstract in df['abstract']:\n",
                "    all_words.extend(simple_tokenize(abstract))\n",
                "\n",
                "word_counts = Counter(all_words)\n",
                "print(f\"Total words: {len(all_words):,}\")\n",
                "print(f\"Unique words: {len(word_counts):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top words (many will be stopwords)\n",
                "top_words = word_counts.most_common(50)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 10))\n",
                "words = [w[0] for w in top_words]\n",
                "counts = [w[1] for w in top_words]\n",
                "\n",
                "ax.barh(range(len(words)), counts, color='steelblue')\n",
                "ax.set_yticks(range(len(words)))\n",
                "ax.set_yticklabels(words)\n",
                "ax.invert_yaxis()\n",
                "ax.set_xlabel('Frequency')\n",
                "ax.set_title('Top 50 Words (Before Stopword Removal)', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify potential stopwords (high frequency, low information)\n",
                "# These are common Indonesian and English stopwords that appear frequently\n",
                "common_stopwords = {\n",
                "    'yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'dengan', 'untuk',\n",
                "    'pada', 'adalah', 'dalam', 'tidak', 'akan', 'juga', 'atau', 'ada',\n",
                "    'the', 'and', 'of', 'to', 'in', 'is', 'for', 'on', 'with', 'that',\n",
                "    'this', 'are', 'as', 'by', 'be', 'an', 'was', 'at', 'or', 'from',\n",
                "}\n",
                "\n",
                "# Count stopwords in top 50\n",
                "top_50_words = set(w[0] for w in top_words)\n",
                "stopwords_in_top = top_50_words & common_stopwords\n",
                "\n",
                "print(f\"Common stopwords found in top 50: {len(stopwords_in_top)}\")\n",
                "print(f\"Words: {sorted(stopwords_in_top)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Language Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple language detection\n",
                "def detect_language(text):\n",
                "    if pd.isna(text) or text == '':\n",
                "        return 'unknown'\n",
                "    \n",
                "    text_lower = str(text).lower()\n",
                "    \n",
                "    id_words = ['yang', 'dan', 'dengan', 'untuk', 'adalah', 'dalam', 'pada', 'dari', 'ini', 'itu']\n",
                "    en_words = ['the', 'and', 'with', 'for', 'this', 'that', 'from', 'are', 'is', 'of']\n",
                "    \n",
                "    id_count = sum(1 for w in id_words if f' {w} ' in f' {text_lower} ')\n",
                "    en_count = sum(1 for w in en_words if f' {w} ' in f' {text_lower} ')\n",
                "    \n",
                "    if id_count > en_count + 2:\n",
                "        return 'Indonesian'\n",
                "    elif en_count > id_count + 2:\n",
                "        return 'English'\n",
                "    else:\n",
                "        return 'Mixed/Unknown'\n",
                "\n",
                "df['detected_lang'] = df['abstract'].apply(detect_language)\n",
                "lang_dist = df['detected_lang'].value_counts()\n",
                "\n",
                "print(\"Language distribution:\")\n",
                "for lang, count in lang_dist.items():\n",
                "    print(f\"  {lang}: {count:,} ({count/len(df)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize language distribution\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "\n",
                "colors = ['#3498db', '#e74c3c', '#95a5a6']\n",
                "ax.pie(lang_dist.values, labels=lang_dist.index, autopct='%1.1f%%', \n",
                "       colors=colors[:len(lang_dist)], startangle=90)\n",
                "ax.set_title('Language Distribution', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Subject Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze subjects if available\n",
                "if 'subjects' in df.columns:\n",
                "    subjects_data = df[df['subjects'].notna() & (df['subjects'] != '')]\n",
                "    \n",
                "    all_subjects = []\n",
                "    for subj_str in subjects_data['subjects']:\n",
                "        subjects = [s.strip() for s in str(subj_str).split(';') if s.strip()]\n",
                "        all_subjects.extend(subjects)\n",
                "    \n",
                "    subject_counts = Counter(all_subjects)\n",
                "    \n",
                "    print(f\"Records with subjects: {len(subjects_data):,} ({len(subjects_data)/len(df)*100:.1f}%)\")\n",
                "    print(f\"Unique subjects: {len(subject_counts):,}\")\n",
                "    \n",
                "    # Top subjects\n",
                "    print(\"\\nTop 15 subjects:\")\n",
                "    for subj, count in subject_counts.most_common(15):\n",
                "        print(f\"  {count:5d}: {subj[:60]}...\" if len(subj) > 60 else f\"  {count:5d}: {subj}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Preprocessing Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"ðŸ“‹ PREPROCESSING RECOMMENDATIONS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Language\n",
                "primary_lang = lang_dist.idxmax()\n",
                "print(f\"\\n1. PRIMARY LANGUAGE: {primary_lang}\")\n",
                "print(f\"   â†’ Use Indonesian NLP tools (Sastrawi stemmer & stopwords)\")\n",
                "if lang_dist.get('English', 0) > len(df) * 0.1:\n",
                "    print(f\"   â†’ Also include English stopwords (significant English content)\")\n",
                "\n",
                "# Text length\n",
                "avg_words = df['abstract_words'].mean()\n",
                "print(f\"\\n2. DOCUMENT LENGTH:\")\n",
                "print(f\"   â†’ Average abstract: {avg_words:.0f} words\")\n",
                "print(f\"   â†’ Recommended min_doc_length: 10-20 tokens\")\n",
                "\n",
                "# Vocabulary\n",
                "print(f\"\\n3. VOCABULARY:\")\n",
                "print(f\"   â†’ Total unique words: {len(word_counts):,}\")\n",
                "print(f\"   â†’ Recommended no_below: 5-10 (remove rare words)\")\n",
                "print(f\"   â†’ Recommended no_above: 0.5 (remove very common words)\")\n",
                "\n",
                "# Additional\n",
                "print(f\"\\n4. ADDITIONAL RECOMMENDATIONS:\")\n",
                "print(f\"   â†’ Enable bigram/trigram detection\")\n",
                "print(f\"   â†’ Track stemming performance (may be slow)\")\n",
                "print(f\"   â†’ Min word length: 3 characters\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean up temporary columns\n",
                "temp_cols = ['title_words', 'abstract_words', 'abstract_chars', 'detected_lang']\n",
                "df_clean = df.drop(columns=temp_cols, errors='ignore')\n",
                "\n",
                "print(f\"\\nâœ… EDA Complete!\")\n",
                "print(f\"ðŸ“Š Dataset ready: {len(df_clean):,} records\")\n",
                "print(f\"\\nðŸ‘‰ Next: Run 03_preprocessing.ipynb for text preprocessing\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}