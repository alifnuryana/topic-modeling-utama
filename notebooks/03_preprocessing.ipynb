{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Text Preprocessing\n",
                "\n",
                "This notebook preprocesses the cleaned text data for topic modeling.\n",
                "\n",
                "## Preprocessing Steps\n",
                "- Text cleaning (case, punctuation, numbers)\n",
                "- Tokenization\n",
                "- Stopword removal (Indonesian + English)\n",
                "- Stemming with PySastrawi (with performance tracking)\n",
                "- Bigram/Trigram phrase detection\n",
                "- Save processed corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import sys\n",
                "import time\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import Counter\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path.cwd().parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src.config import get_settings, ensure_directories\n",
                "from src.preprocessor import IndonesianPreprocessor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load settings and data\n",
                "settings = get_settings()\n",
                "ensure_directories(settings)\n",
                "\n",
                "data_path = settings.processed_data_dir / settings.clean_metadata_file\n",
                "print(f\"Loading cleaned data from: {data_path}\")\n",
                "\n",
                "df = pd.read_csv(data_path)\n",
                "print(f\"Loaded {len(df):,} records\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize Preprocessor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "USE_STEMMING = True  # Set to False to skip stemming (much faster)\n",
                "USE_BIGRAMS = True\n",
                "USE_TRIGRAMS = True\n",
                "\n",
                "# Custom stopwords (domain-specific)\n",
                "CUSTOM_STOPWORDS = {\n",
                "    # Academic terms that appear in most papers\n",
                "    'penelitian', 'hasil', 'metode', 'data', 'analisis',\n",
                "    'kesimpulan', 'saran', 'pembahasan', 'bab', 'tabel',\n",
                "    'gambar', 'lampiran', 'daftar', 'pustaka', 'referensi',\n",
                "    # English academic terms\n",
                "    'research', 'result', 'method', 'data', 'analysis',\n",
                "    'conclusion', 'table', 'figure', 'chapter',\n",
                "}\n",
                "\n",
                "print(f\"Configuration:\")\n",
                "print(f\"  Stemming: {USE_STEMMING}\")\n",
                "print(f\"  Bigrams: {USE_BIGRAMS}\")\n",
                "print(f\"  Trigrams: {USE_TRIGRAMS}\")\n",
                "print(f\"  Custom stopwords: {len(CUSTOM_STOPWORDS)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize preprocessor\n",
                "preprocessor = IndonesianPreprocessor(\n",
                "    settings=settings,\n",
                "    custom_stopwords=CUSTOM_STOPWORDS,\n",
                "    use_stemming=USE_STEMMING,\n",
                "    use_bigrams=USE_BIGRAMS,\n",
                "    use_trigrams=USE_TRIGRAMS,\n",
                ")\n",
                "\n",
                "print(f\"Total stopwords: {len(preprocessor.stopwords)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Test Preprocessing on Sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on a sample abstract\n",
                "sample_idx = df[df['abstract'].str.len() > 200].sample(1).index[0]\n",
                "sample_text = df.loc[sample_idx, 'abstract']\n",
                "\n",
                "print(\"Sample Abstract:\")\n",
                "print(\"-\" * 60)\n",
                "print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess sample\n",
                "start_time = time.time()\n",
                "sample_tokens = preprocessor.preprocess_text(sample_text, apply_phrases=False)\n",
                "elapsed = time.time() - start_time\n",
                "\n",
                "print(f\"\\nPreprocessed tokens ({len(sample_tokens)} tokens, {elapsed:.3f}s):\")\n",
                "print(\"-\" * 60)\n",
                "print(sample_tokens[:30])\n",
                "if len(sample_tokens) > 30:\n",
                "    print(f\"... and {len(sample_tokens) - 30} more\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Process All Documents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get abstracts\n",
                "abstracts = df['abstract'].fillna('').tolist()\n",
                "\n",
                "print(f\"Processing {len(abstracts):,} documents...\")\n",
                "print(f\"This may take a while, especially with stemming enabled.\")\n",
                "print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Process all documents\n",
                "start_time = time.time()\n",
                "\n",
                "processed_docs = preprocessor.preprocess_documents(\n",
                "    abstracts,\n",
                "    fit_phrases=USE_BIGRAMS or USE_TRIGRAMS,\n",
                "    show_progress=True,\n",
                ")\n",
                "\n",
                "total_time = time.time() - start_time\n",
                "print(f\"\\n‚úÖ Preprocessing complete in {total_time:.1f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display statistics\n",
                "stats = preprocessor.stats\n",
                "\n",
                "print(\"\\nPreprocessing Statistics:\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Total documents:      {stats.total_documents:,}\")\n",
                "print(f\"Processed documents:  {stats.processed_documents:,}\")\n",
                "print(f\"Skipped documents:    {stats.skipped_documents:,}\")\n",
                "print(f\"Total tokens:         {stats.total_tokens:,}\")\n",
                "print(f\"Unique tokens:        {stats.unique_tokens:,}\")\n",
                "print(f\"Avg tokens/doc:       {stats.avg_tokens_per_doc:.1f}\")\n",
                "print(f\"Total time:           {stats.total_time_seconds:.1f}s\")\n",
                "if stats.stemming_time_seconds > 0:\n",
                "    print(f\"Stemming time:        {stats.stemming_time_seconds:.1f}s ({stats.stemming_time_seconds/stats.total_time_seconds*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Analyze Processed Corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Token length distribution\n",
                "token_counts = [len(doc) for doc in processed_docs]\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 5))\n",
                "ax.hist(token_counts, bins=50, edgecolor='white', alpha=0.7)\n",
                "ax.axvline(np.median(token_counts), color='red', linestyle='--',\n",
                "           label=f'Median: {np.median(token_counts):.0f}')\n",
                "ax.set_xlabel('Tokens per Document')\n",
                "ax.set_ylabel('Frequency')\n",
                "ax.set_title('Document Length Distribution (After Preprocessing)', fontweight='bold')\n",
                "ax.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top tokens after preprocessing\n",
                "all_tokens = [token for doc in processed_docs for token in doc]\n",
                "token_freq = Counter(all_tokens)\n",
                "\n",
                "print(f\"\\nTop 30 tokens after preprocessing:\")\n",
                "print(\"-\" * 50)\n",
                "for token, count in token_freq.most_common(30):\n",
                "    print(f\"  {count:6d}: {token}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for bigrams/trigrams\n",
                "phrases = [t for t in token_freq if '_' in t]\n",
                "print(f\"\\nDetected phrases (bigrams/trigrams): {len(phrases)}\")\n",
                "\n",
                "if phrases:\n",
                "    print(\"\\nTop 20 phrases:\")\n",
                "    phrase_counts = {p: token_freq[p] for p in phrases}\n",
                "    for phrase, count in sorted(phrase_counts.items(), key=lambda x: -x[1])[:20]:\n",
                "        print(f\"  {count:5d}: {phrase}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create DataFrame with Tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter original dataframe to match processed documents\n",
                "# (some documents may have been skipped due to short length)\n",
                "\n",
                "# Find valid indices\n",
                "valid_indices = []\n",
                "doc_idx = 0\n",
                "\n",
                "for i, abstract in enumerate(abstracts):\n",
                "    tokens = preprocessor.preprocess_text(abstract, apply_phrases=False)\n",
                "    if len(tokens) >= settings.min_doc_length:\n",
                "        valid_indices.append(i)\n",
                "\n",
                "# Create result dataframe\n",
                "df_processed = df.iloc[valid_indices].copy().reset_index(drop=True)\n",
                "df_processed['tokens'] = processed_docs\n",
                "df_processed['token_count'] = [len(doc) for doc in processed_docs]\n",
                "\n",
                "print(f\"Processed dataframe: {len(df_processed):,} records\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview\n",
                "df_processed[['title', 'token_count', 'tokens']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save processed corpus\n",
                "corpus_path = settings.processed_data_dir / settings.processed_corpus_file\n",
                "\n",
                "corpus_data = {\n",
                "    'documents': processed_docs,\n",
                "    'dataframe': df_processed,\n",
                "    'stats': preprocessor.stats,\n",
                "}\n",
                "\n",
                "with open(corpus_path, 'wb') as f:\n",
                "    pickle.dump(corpus_data, f)\n",
                "\n",
                "print(f\"‚úÖ Saved processed corpus to: {corpus_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save preprocessor (for consistent preprocessing of new text)\n",
                "preprocessor_path = settings.processed_data_dir / 'preprocessor.pkl'\n",
                "preprocessor.save(preprocessor_path)\n",
                "print(f\"‚úÖ Saved preprocessor to: {preprocessor_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Also save as CSV (without tokens for readability)\n",
                "csv_path = settings.processed_data_dir / 'processed_metadata.csv'\n",
                "df_processed.drop(columns=['tokens']).to_csv(csv_path, index=False)\n",
                "print(f\"‚úÖ Saved metadata CSV to: {csv_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"PREPROCESSING COMPLETE\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nüìä Documents processed: {len(processed_docs):,}\")\n",
                "print(f\"üìù Unique tokens: {stats.unique_tokens:,}\")\n",
                "print(f\"üìà Avg tokens/doc: {stats.avg_tokens_per_doc:.1f}\")\n",
                "print(f\"‚è±Ô∏è  Total time: {stats.total_time_seconds:.1f}s\")\n",
                "\n",
                "print(f\"\\nüìÅ Output files:\")\n",
                "print(f\"   - {corpus_path}\")\n",
                "print(f\"   - {preprocessor_path}\")\n",
                "print(f\"   - {csv_path}\")\n",
                "\n",
                "print(f\"\\nüëâ Next: Run 04_lda_modeling.ipynb to train the LDA model\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}